<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling">
    <meta property="og:title" content="REVERSE" />
    <meta property="og:description" content="Large Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, Visual Haystacks (VHs). We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU -- far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs." />
    <meta property="og:url" content="http://reverse-vlm.github.io" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
    <meta property="og:image" content="/static/images/REVERSE_logo.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling">
    <meta name="twitter:description" content="Large Multimodal Models (LMMs) have made significant strides in visual question-answering for single images. Recent advancements like long-context LMMs have allowed them to ingest larger, or even multiple, images. However, the ability to process a large number of visual tokens does not guarantee effective retrieval and reasoning for multi-image question answering (MIQA), especially in real-world applications like photo album searches or satellite imagery analysis. In this work, we first assess the limitations of current benchmarks for long-context LMMs. We address these limitations by introducing a new vision-centric, long-context benchmark, Visual Haystacks (VHs). We comprehensively evaluate both open-source and proprietary models on VHs, and demonstrate that these models struggle when reasoning across potentially unrelated images, perform poorly on cross-image reasoning, as well as exhibit biases based on the placement of key information within the context window. Towards a solution, we introduce MIRAGE (Multi-Image Retrieval Augmented Generation), an open-source, lightweight visual-RAG framework that processes up to 10k images on a single 40G A100 GPU -- far surpassing the 1k-image limit of contemporary models. MIRAGE demonstrates up to 13% performance improvement over existing open-source LMMs on VHs, sets a new state-of-the-art on the RetVQA multi-image QA benchmark, and achieves competitive performance on single-image QA with state-of-the-art LMMs." />
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
    <meta name="twitter:image" content="static/images/REVERSE_logo.png">
    <meta name="twitter:card" content="REVERSE Project Logo: A cartoon character photo sitting on top of a haystack of images.">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords"
        content="Large Multimodal Models, Long-context Reasoning, VQA, Image Retrieval">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon_io/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon_io">
    <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="static/images/favicon_io/site.webmanifest">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/fontawesome/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/fontawesome/js/fontawesome.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered is-vcentered">
                    <div class="column "><img src="static/images/vhs_logo.png" height="187" width="187"></div>
                    <div class="column has-text-centered is-four-fifths is-vcentered">
                        <h1 class="title is-2 publication-title">Generate, but Verify: Reducing Visual Hallucination in Vision-Language Models with Retrospective Resampling</h1>
                    </div>
                </div>
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered is-four-fifths">
                            <div class="is-size-5 publication-authors">
                                <!-- Paper authors -->
                                <span class="author-block"><a href="https://tsunghan-wu.github.io/" target="_blank">Tsung-Han Wu<sup>1</sup></a>, </span>
                                <span class="author-block"><a href="https://linkedin.com/in/heekyung-lee-624753289" target="_blank">Heekyung Lee<sup>1,2</sup></a>, </span>
                                <span class="author-block"><a href="https://jiaxin.ge" target="_blank">Jiaxin Ge<sup>1</sup></a></span>
                                <br>
                                <span class="author-block"><a href="https://people.eecs.berkeley.edu/~jegonzal/" target="_blank">Joseph E. Gonzalez<sup>1</sup></a>, </span>
                                <span class="author-block"><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell<sup>1</sup></a>, </span>
                                <span class="author-block"><a href="https://dchan.cc/" target="_blank">David M. Chan<sup>1</sup></a></span>
                            </div>

                            <div class="is-size-5 publication-authors">
                                <span class="author-block"><sup>1</sup>UC Berkeley</span>
                                <span class="author-block"><sup>2</sup>POSTECH</span>
                            </div>

                            <div class="column has-text-centered">
                                <div class="publication-links">

                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2407.13766" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                        <a href="https://reverse-vlm.github.io" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon" style="vertical-align: middle; font-size: 20px;">✨</span>
                                            <span>Demo</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                        <a href="https://github.com/visual-haystacks/vhs_benchmark" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fa-brands fa-github"></i>
                                            </span>
                                            <span>Dataset</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                        <a href="https://github.com/visual-haystacks/mirage" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fa-brands fa-github"></i>
                                            </span>
                                            <span>Code/Model</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero ">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">

                    <!-- YouTube Video Link -->
<!--                     <h2 class="title is-4">Video Introduction</h2>
                    <div class="content has-text-justified">
                        <div class="video-wrapper">
                            <iframe src="https://www.youtube.com/embed/PZ7H9vNZZag" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                        </div>
                    </div> -->
                    <h2 class="title is-4">Visual Hallucination is Still a Problem</h2>
                    <div class="content has-text-justified">
                        <p>
                            Vision-Language Models (VLMs) have made huge strides on benchmarks like image captioning and VQA. But they still hallucinate — generating plausible-sounding text that's clearly not grounded in the image.
                            To mitigate this, we introduce <strong>REVERSE</strong> (<strong>RE</strong>trospective <strong>VER</strong>ification and <strong>SE</strong>lf-correction), a unified framework that trains a VLM to both <strong>detect</strong> and <strong>fix</strong> its own hallucinations on-the-fly.
                        </p>
                    </div>

                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->



    <!-- SESAME -->
    <section class="section ">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">
                    <h2 class="title is-4">Why REVERSE?</h2>
                </div>
            </div>
            <!-- <div id="results-carousel" class="carousel results-carousel"> -->
                <div class="content has-text-justified">
                    <p>Previous approaches fall into two camps:
                    <ul>
                        <li>
                            <b>Generation adjustment</b>: Uses custom losses or decoding tweaks to reduce hallucination. But these can't fix errors after generation.
                        </li>
                        <li>
                            <b>Post-hoc verification</b>: Adds an external verifier (e.g., GPT-4) after generation. Accurate, but slow, complex, and often leads to generic refusals.
                        </li>
                    </ul>
                    <p>
                        <strong>REVERSE</strong> takes a different route: ✅ A single VLM that detects, backtracks, and corrects hallucinations during decoding.
                    </p>
                </div>
                <!-- <div class="item is-vcentered">
                    <img src="static/images/figures/fig1.png" alt="VHs dataset overview" />
                </div> -->
            <!-- </div> -->
        </div>
    </section>

    <section class="section hero">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">
                    <h2 class="title is-4">How It Works: Three Key Ingredients</h2>
                    <div class="content has-text-justified">
                        <ul>
                            <li>
                                <b>Explicit Confidence Estimation</b> 
                                <br>
                                VLMs often rely on token probabilities as a proxy for confidence — but those are unreliable and poorly calibrated. So we train the model to explicitly tag phrase-level confidence using new tokens:
                                <ul>
                                    <li>
                                        <b>&lt;SPAN&gt;</b>: Start of a key phrase
                                    </li>
                                    <li>
                                        <b>&lt;/CN&gt;</b>: End of confident (grounded) phrase
                                    </li>
                                    <li>
                                        <b>&lt;/UN&gt;</b>: End of unconfident (potentially hallucinated) phrase
                                    </li>
                                </ul>
                            </li>
                            <br>
                            <p>
                                This lets the model not only flag uncertainty but also decide where to backtrack. We end up building a 1.3M-sample instruction-tuning dataset, augmenting LLaVA-v1.5, to teach the model this skill. Ensuring the quality of this dataset includes designing how to segment text into phrases, injecting realistic hallucinations, and even handling multilingual cases — not trivial (details are in our paper)!
                            </p>
                        </ul>


                    </div>
                    <div class="content has-text-justified">
                        <ul>
                            <li>
                                <b>Hallucination-Aware Training</b>
                                <br>
                                Our training loss has three objectives:
                                <ul>
                                    <li>
                                        <b>Standard next token prediction</b>: Standard VLM objective for the correct captioning / VQA data
                                    </li>
                                    <li>
                                        <b>Confidence tagging</b>: (learn when to emit &lt;/UN&gt; vs &lt;/CN&gt;)
                                    </li>
                                    <li>
                                        <b>Avoiding hallucination modeling</b>: model sees hallucinated inputs but is not trained to generate them
                                </ul>
                            </li>
                            <br>
                            <p>
                                So the loss is applied like the following animation (please see our paper for more details):
                            </p>
                        </ul>
                    </div>
                    <!-- <div class="item is-vcentered">
                        <img src="static/images/figures/fig2.png" alt="Effectiveness of LMMs with increasing number of images" width="90%">
                    </div> -->

                    <div class="content has-text-justified">
                        <ul>
                            <li>
                                <b>Retrospective Resampling</b>
                                <br>
                                When the model predicts an unconfident phrase (marked with <code>&lt;/UN&gt;</code>), REVERSE activates a verification loop to resample and correct the hallucination:
                                <li>
                                    <strong>Backtrack:</strong> Roll back to the latest <code>&lt;/CN&gt;</code> tag, or further if needed, once the model’s unconfidence exceeds a threshold.
                                </li>
                                <li>
                                    <strong>Correction:</strong> The model uses two techniques in tandem:
                                    <ul>
                                        <li>
                                            strong>Rejection Sampling:</strong> Temperature is increased to encourage diverse alternatives; unconfident outputs are rejected.</li>
                                        </li>
                                        <li><strong>Query Rewriting:</strong> The user query is augmented with hints pointing out the potential hallucination to discourage repetition.</li>
                                    </ul>
                                </li>
                                <li><strong>Regeneration:</strong> Generation resumes from the backtracked point, continuing until a confident phrase is produced or retry limits are hit.</li>
                            </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{wu2024visual,
  title={Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling},
  author={Wu, Tsung-Han and Lee, Heekyung and Ge, Jiaxin and Gonzalez, Joseph E and Darrell, Trevor and Chan, David M},
  journal={arXiv},
  year={2025},
  url={TODO}
}</code></pre>
        </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in
                            the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>
