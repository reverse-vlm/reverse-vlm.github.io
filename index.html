<!DOCTYPE html>
<html lang="en"></html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling">
    <meta property="og:title" content="REVERSE" />
    <meta property="og:description" content="Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 20% on CHAIR-MSCOCO and 50% on Halo Quest." />
    <meta property="og:url" content="http://reverse-vlm.github.io" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
    <meta property="og:image" content="/static/images/REVERSE_logo.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling">
    <meta name="twitter:description" content="Vision-Language Models (VLMs) excel at visual understanding but often suffer from visual hallucinations, where they generate descriptions of nonexistent objects, actions, or concepts, posing significant risks in safety-critical applications. Existing hallucination mitigation methods typically follow one of two paradigms: generation adjustment, which modifies decoding behavior to align text with visual inputs, and post-hoc verification, where external models assess and correct outputs While effective, generation adjustment methods often rely on heuristics and lack correction mechanisms, while post-hoc verification is complicated typically requiring multiple models and tending to reject outputs rather than refine them. In this work, we introduce REVERSE, a unified framework that integrates hallucination-aware training with on-the-fly self verification. By leveraging a new hallucination-verification dataset containing over 1.3M semi-synthetic samples, along with a novel inference-time retrospective resampling technique, our approach enables VLMs to both detect hallucinations during generation and dynamically revise those hallucinations. Our evaluations show that REVERSE achieves state-of-the-art hallucination reduction, outperforming the best existing methods by up to 20% on CHAIR-MSCOCO and 50% on Halo Quest." />
    <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
    <meta name="twitter:image" content="static/images/REVERSE_logo.png">
    <meta name="twitter:card" content="REVERSE Project Logo: A cartoon llava character holding a tool that fixes hallucination.">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords"
        content="Large Multimodal Models, Hallucinations, Retrospective Resampling">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon_io/favicon.ico">
    <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./static/images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="./static/images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="static/images/favicon_io/site.webmanifest">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/fontawesome/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/fontawesome/js/fontawesome.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered is-vcentered">
                    <div class="column "><img src="static/images/REVERSE_logo.png" height="700" width="700"></div>
                    <div class="column has-text-centered is-four-fifths is-vcentered">
                        <h1 class="title is-2 publication-title">Generate, but Verify: Reducing Visual Hallucination in Vision-Language Models with Retrospective Resampling</h1>
                    </div>
                </div>
                <div class="container is-max-desktop">
                    <div class="columns is-centered">
                        <div class="column has-text-centered is-four-fifths">
                            <div class="is-size-5 publication-authors">
                                <!-- Paper authors -->
                                <span class="author-block"><a href="https://tsunghan-wu.github.io/" target="_blank">Tsung-Han Wu<sup>1</sup></a>, </span>
                                <span class="author-block"><a href="https://linkedin.com/in/heekyung-lee-624753289" target="_blank">Heekyung Lee<sup>1,2</sup></a>, </span>
                                <span class="author-block"><a href="https://jiaxin.ge" target="_blank">Jiaxin Ge<sup>1</sup></a></span>
                                <br>
                                <span class="author-block"><a href="https://people.eecs.berkeley.edu/~jegonzal/" target="_blank">Joseph E. Gonzalez<sup>1</sup></a>, </span>
                                <span class="author-block"><a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell<sup>1</sup></a>, </span>
                                <span class="author-block"><a href="https://dchan.cc/" target="_blank">David M. Chan<sup>1</sup></a></span>
                            </div>

                            <div class="is-size-5 publication-authors">
                                <span class="author-block"><sup>1</sup>UC Berkeley</span>
                                <span class="author-block"><sup>2</sup>POSTECH</span>
                            </div>

                            <div class="column has-text-centered">
                                <div class="publication-links">

                                    <span class="link-block">
                                        <a href="https://arxiv.org/abs/2407.13766" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="ai ai-arxiv"></i>
                                            </span>
                                            <span>arXiv</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                        <a href="https://reverse-vlm.github.io" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon" style="vertical-align: middle; font-size: 20px;">✨</span>
                                            <span>Demo</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                        <!-- TODO: Dataset -->
                                        <a href="https://github.com/visual-haystacks/vhs_benchmark" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fa-brands fa-github"></i>
                                            </span>
                                            <span>Dataset</span>
                                        </a>
                                    </span>

                                    <span class="link-block">
                                        <!-- TODO: Code/Model -->
                                        <a href="https://github.com/visual-haystacks/mirage" target="_blank"
                                            class="external-link button is-normal is-rounded is-dark">
                                            <span class="icon">
                                                <i class="fa-brands fa-github"></i>
                                            </span>
                                            <span>Code/Model</span>
                                        </a>
                                    </span>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero ">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">

                    <h2 class="title is-4">VLMs suffer from Hallucinations </h2>
                    <div class="content has-text-justified">
                        <p>
                            Vision-Language Models (VLMs) have made huge strides on tasks like image captioning or visual question answering. However, they still suffer from hallucinations, where they generate descriptions on nonexistent objects or concepts.
                        </p>
                    </div>
                    <div class="content has-text-justified">
                        <p><strong>Previous approaches generally fall into two paradigms:</strong></p>
                        <!-- <div class="item is-vcentered">
                            <img src="static/images/intro_2.png" alt="How it works" />
                        </div>  -->
                        <ul>
                            <li>
                                <strong>Generation Adjustment</strong>: This method aims to improve the alignment of textual outputs with visual inputs by modifying the 
                                VLM’s generation process. This can be done either in a <em>training-free</em> manner (adjusting logits at <em>decoding time</em>) 
                                or through a <em>training-based</em> approach (introducing additional supervision signals or custom objective functions).
                            </li>
                            <li>
                                <strong>Post-hoc Verification</strong>: This method introduces large external models (e.g., GPT-4) to 
                                <em>evaluate and verify outputs</em> after generation.
                            </li>
                        </ul>
                        <p>
                            However, <b>generation adjustment methods</b> struggle to correct erroneous tokens <em>once generated</em> and do not leverage 
                            retrospective reasoning to assess output quality. On the other hand, <b>post-hoc verification</b> is computationally expensive, 
                             and often results in <em>generic refusals</em> rather than targeted improvements.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->

    <!-- REVERSE -->
    <section class="section ">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">
                    <h2 class="title is-4">REVERSE: REtrospective VERification and SElf-correction </h2>
                </div>
            </div>
            <p>
                To mitigate this, we introduce <strong>REVERSE</strong> (<strong>RE</strong>trospective <strong>VER</strong>ification and <strong>SE</strong>lf-correction), the first framework to integrate generation adjustment with online 
                post-hoc verification within a single VLM architecture. REVERSE <b>detects</b>, <b>backtracks</b>, and <b>corrects hallucinations</b> during the decoding process.
            </p>
            <video class="is-fullwidth" autoplay loop muted playsinline>
                <!-- <source src="./static/images/website_figures_final.mp4" type="video/mp4"> -->
                <!-- <source src="./static/images/fig1_final.mp4" type="video/mp4"> -->
                <source src="./static/images/fig1_only_reverse.mp4" type="video/mp4">
                <h2 class="title is-4">REVERSE: REtrospective VERification and SElf-correction </h2>
                <!-- TODO: diff source? -->
                Your browser does not support the video tag.
            </video>

            <div class="columns is-centered has-text-centered">
                <div class="column is-full">
                    <h2 class="title is-4">A New Training Dataset </h2>
                </div>
            </div>
            <div class="content has-text-justified">
                <ul>
                    <li>
                        <b>Introducing Three Special Tokens</b>: REVERSE training introduces three tokens which can be used to explicity mark key phrases to represent the model's confidence level:
                        <br>
                        <ul>
                            <li>
                                <code>&lt;/SPAN&gt;</code>: Start of a key phrase
                            </li>
                            <li>
                                <code>&lt;/CN&gt;</code>: End of confident (grounded) phrase
                            </li>
                            <li>
                                <code>&lt;/UN&gt;</code>: End of unconfident (potentially hallucinated) phrase
                            </li>
                        </ul>
                    </li>
                    <br>
                    <p>
                        These tokens, when placed before or after phrases in a scene, serve as ad-hoc classifiers for the model's confidence. 
                        This enables the model not only to flag uncertainty but also to determine  <em>where</em> to backtrack.
                    </p>
                </ul>
                <ul>
                    <li>
                        <b>A New Training Dataset</b>:
                            Annotating our data with such tokens, we built <strong>1.3M-sample instruction-tuning dataset</strong>, augmenting LLaVA-v1.5.
                            Our dataset maintains a similar overall composition from the LLaVA-v1.5. dataset, while preserving the data quality, the same average question-answer pairs per sample and a comparable question type distribution.
                    </li> 
                    <!-- TODO: Dataset example picture -->
                </ul>
            </div>
            <div class="item is-vcentered">
                <!-- <img src="static/images/dataset_example.png" alt="" /> -->
                <img src="static/images/dataset_example2.png" alt="" />
            </div>
        </div>
    </section>

    <section class="section hero">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">
                    <h2 class="title is-4">How it works: Hallucination Aware Training</h2>
                    <div class="content has-text-justified">
                            <ul>
                                Three Objectives of training loss:
                                <ul>
                                    <li>
                                        <b>Standard next token prediction</b>: We enable conventional instruction tuning to allow VLMs to perform next token prediction to generate accurate answers.
                                    </li>
                                    <li>
                                        <b>Avoiding hallucination modeling</b>: We reduce the likelihood of generating the hallucinated tokens that we have introduced in the new dataset.
                                    </li>
                                    <li>
                                        <b>Confidence tagging</b>: 
                                        <!-- (learn when to emit &lt;/UN&gt; vs &lt;/CN&gt;) -->
                                        The model learns when to generate <b>&lt;SPAN&gt;</b> at the start of the key phrases, and <b>&lt;CN&gt;</b> or <b>&lt;UN&gt;</b> as explicit confidence estimators around those phrases.
                                    </li>
                                </ul>
                            <p>
                                We achieve all of these goals by assigning a weight to each token during training: positive weights to<b>&lt;SPAN&gt;</b> and <b>&lt;CN&gt;</b>, zero-weights are assigned to tokens within <b>&lt;SPAN&gt;</b> and <b>&lt;UN&gt;</b>(i.e., masking out the targets) to avoid impacting the likelihood when training on ungrounded data.
                            </p>
                        </ul>
                    </div>
                    <!-- TODO: perhaps put Figure 3 -->
                 
                    <h2 class="title is-4">How it works: Retrospective Resampling</h2>
                    <div class="content has-text-justified">
                        <ul>
                            <!-- <b>Retrospective Resampling</b> -->
                            During inference, the model follows standard next-token prediction but continuously monitors the likelihood of <b>&lt;UN&gt;</b>, triggering retrospective resampling when that likelihood exceeds a pre-defined threshold.
                            When the model predicts an unconfident phrase (marked with <b>&lt;UN&gt;</b>), REVERSE activates a verification loop to resample and correct the hallucination: <br>

                            <li>
                                <strong>Backtracking Strategies:</strong> Roll back to the latest <b>&lt;CN&gt;</b> tag, or further if needed, once the model’s unconfidence exceeds a threshold.
                            </li>
                            <li>
                                <strong>Correction Strategies:</strong> The model uses two techniques for self-correction:
                                <ul>
                                    <li>
                                        <strong>Rejection Sampling:</strong> We refine uncertain phrases by resampling multiple times at an increased temperature, seeking an alternative lower than the threshold.</li>
                                    </li>
                                    <li>
                                        <strong>Query Rewriting:</strong> We augment the user query with hints pointing out the potential hallucination to discourage repetition. This improves the model's ability to recognize the hint, providing stronger sginals for the model to do self-correction.
                                        <!-- query sys prompt  -->
                                    </li>
                                </ul>
                            </li>
                        </ul>
                     <br>
                    </div>
                </div>
            </div>
            <video class="is-fullwidth" autoplay loop muted playsinline>
                <source src="./static/images/fig3_animation.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>

    <section class="section ">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full">
                    <h2 class="title is-4">Examples</h2>
                    <div class="content has-text-justified">
                        <p> Examples from AMBER benchmark evaluation for comparison. Hallucinated objects from other VLMs are highlighted in red. </p>
                        <div class="item is-vcentered">
                            <img src="static/images/qualitative_examples_2.png" alt="" />
                        </div>
                        <h2 class="title is-4">Results</h2>
                        <b> Image Captioning Tasks</b>
                        <div class="item is-vcentered">
                            <img src="static/images/results.png" alt="" />
                        </div>
                        <b> Open-ended Question Answering </b>
                        <div class="item is-vcentered">
                            <img src="static/images/results.png" alt="" />
                        </div>
                        <b> Discriminative Questions </b>
                        <div class="item is-vcentered">
                            <img src="static/images/results.png" alt="" />
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!--BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{wu2024visual,
  title={Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling},
  author={Wu, Tsung-Han and Lee, Heekyung and Ge, Jiaxin and Gonzalez, Joseph E and Darrell, Trevor and Chan, David M},
  journal={arXiv},
  year={2025},
  url={TODO}
}</code></pre>
        </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a
                                href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in
                            the footer. <br> This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>
